# Hyperparameters for training

lr0: 0.01  # initial learning rate (SGD=5E-3 Adam=5E-4)
lrf: 0.01  # final OneCycleLR learning rate (lr0 * lrf)
momentum: 0.937  # SGD momentum
weight_decay: 0.0005  # optimizer weight decay
d_model: 512
nhead: 8
num_encoder_layers: 6
num_decoder_layers: 6
dim_feedforward: 2048
dropout: 0.1
activation: "relu"
normalize_before: False
return_intermediate_dec: False